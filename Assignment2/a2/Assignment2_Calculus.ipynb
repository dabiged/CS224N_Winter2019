{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Preamble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In word2vec, the conditional probability distribution is given by taking vector dot-products and applying the softmax function:\n",
    "\n",
    "$$P(O=o|C=c) =\\frac{e^{(u_o^T v_c)}}{\\sum \\limits_{w\\in Vocab}e^{(u_w^T v_c)}}\\tag 1$$          \n",
    "\n",
    "Here, $u_o$ is the ‘outside’ vector representing outside word $o$, and $v_c$ is the ‘center’ vector representing centerword $c$.  To contain these parameters, we have two matrices, $U$ and $V$.  The columns of $U$ are all the ‘outside’vectors $u_w$.  The columns of $V$ are all of the ‘center’ vectors $v_w$.  Both $U$ and $V$ contain a vector for every $w\\in Vocabulary$.\n",
    "\n",
    "Recall from lectures that, for a single pair of words $c$ and $o$, the loss is given by:\n",
    "\n",
    "$$J_{naive-softmax}(v_c,o,U) = −log P(O=o|C=c) \\tag 2$$.         \n",
    "\n",
    "Another way to view this loss is as the cross-entropy  between the true distributiony and the predicted distribution $\\hat{y}$.  Here, both $y$ and $\\hat{y}$ are vectors with length equal to the number of words in the vocabulary. Furthermore, the $k$th entry in these vectors indicates the conditional probability of the $k$th word being an ‘outside word’ for the given $c$.  The true empirical distribution $y$ is a one-hot vector with a 1 for the true out-side word $o$, and 0 everywhere else.  The predicted distribution $\\hat{y}$ is the probability distribution P(O|C=c)given by our model in equation (1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Question 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Show that the naive-softmax loss given in Equation (2) is the same as the cross-entropy loss between $y$ and $\\hat{y}$; i.e., show that\n",
    "\n",
    "$$-\\sum \\limits_{w \\in Vocab} y_w log(\\hat{y}_w) = -log(\\hat{y}_o) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Since the vector $y_w$ is a one-hot encoded, the summation value is zero for all values of $w$ not equal to $o$, and 1 for $w=o$; thus:\n",
    "\n",
    "$$- \\sum_{w \\in Vocab} y_w \\log(\\hat{y}_w) = -0\\times\\log(\\hat{y_1})-0\\times\\log(\\hat{y_2}) - \\dots -1\\times\\log(\\hat{y_o}) - \\dots - 0\\times \\log(\\hat{y_V})=-\\log(\\hat{y_o})​$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Question 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Compute the partial derivative of $J_{naive-softmax}(v_c,o,U)$ with respect to $v_c$.  Please write your answer in terms of $y$,  $\\hat{y}$, and $U$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Follow the convention of codes, $U \\in \\mathbb{R}^{V\\times N}$, $V$ is the vocabulary size, $N$ is the length of embedding. $y,\\hat{y},v_c,$ are row vectors.\n",
    "\n",
    "From identity $7$ we get: $$ \\frac{\\partial J}{\\partial \\theta} = \\hat{y} - y \\tag3 $$ From identity $2$ we get: $$ \\frac{\\partial \\theta}{\\partial v_c} = U \\tag{4} $$ Combine $(3)$ and $(4)$ : \n",
    "\n",
    "$$ \\frac{\\partial J}{\\partial v_c} = \\frac{\\partial J}{\\partial \\theta} \\frac{\\partial \\theta}{\\partial v_c} = (\\hat{y} - y)U$$ \n",
    "\n",
    "Note: $\\frac{\\partial J}{\\partial v_c} \\in \\mathbb{R}^{1 \\times N},\\text{same shape with } v_c $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Question 3:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Compute the partial derivatives of $J_{naive-softmax}(v_c,o,U)$ with respect to each of the ‘outside’ word vectors, $u_w$’s.  There will be two cases:  when $w=o$, the true ‘outside’ word vector, and $w\\ne o$, for all other words.  Please write you answer in terms of $y$,$\\hat{y}$, and $v_c$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "From identity $5$ we get: \n",
    "\n",
    "$$ \\frac{\\partial J}{\\partial U} = (\\hat{y} - y)v_c \\in \\mathbb{R}^{V \\times N} ,\\text{same shape with } U \\tag{5} $$ \n",
    "\n",
    "then \n",
    "\n",
    "$$ \\frac{\\partial J}{\\partial u_w} = \\text{the } w \\text{ row of } \\frac{\\partial J}{\\partial U} $$ \n",
    "\n",
    "The computing of $J$ involves all the rows of $U$, so it doesn't matter whether $w = o$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The sigmoid function is given by Equation 6: \n",
    "\n",
    "$$\\sigma (x) =\\frac{1}{1+e^{-x}} = \\frac{e^x}{e^x+ 1}\\tag{6}$$\n",
    "\n",
    "Please compute the derivative of $\\sigma(x)$ with respect to $x$, where $x$ is a vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$ \\frac{\\partial \\sigma(x)}{\\partial x} = \\sigma(x)(1-\\sigma(x)) $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now  we  shall  consider  the  Negative  Sampling  loss,  which  is  an  alternative  to  the  NaiveSoftmax loss.  Assume that $K$ negative samples (words) are drawn from the vocabulary.  For simplicity of notation we shall refer to them as $w_1$,$w_2$,..., $w_K$ and their outside vectors as $u_1$,...,$u_K$.  Note that $o \\notin {w1,...,wK}$.  For a center word $c$ and an outside word $o$, the negative sampling loss function is given by: \n",
    "\n",
    "$$J_{neg-sample}(v_c,o,U) =−log(\\sigma(u^T_o v_c))−\\sum \\limits^{K}\\limits_{k=1}log(\\sigma(−u^T_k v_c))\\tag7$$ \n",
    "\n",
    "for a sample $w_1$,...,$w_K$, where $\\sigma(.)$ is the sigmoid function. Please repeat parts (b) and (c), computing the partial derivatives of $J_{neg-sample}$ with respect to $v_c$, with respect  to $u_o$,  and  with  respect  to  a  negative  sample $u_k$.   Please  write  your  answers  in  terms  of  the vectors $u_o$,$v_c$, and $u_k$, where $k \\in [1,K]$.  After you’ve done this, describe with one sentence why this loss function is much more efficient to compute than the naive-softmax loss.  Note, you should be able to use your solution to part (d) to help compute the necessary gradients here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial J_{neg-sample}(v_c,o,U)}{\\partial v_c} = \\frac{\\partial}{\\partial v_c}\\left(−log(\\sigma(u^T_o v_c))−\\sum \\limits^{K}\\limits_{k=1}log(\\sigma(−u^T_k v_c))\\right)$$\n",
    "\n",
    "Invoke the Chain rule for the 1st time:\n",
    "\n",
    "Let $$ s_1 = u_o^T v_c, s_2 = u_k^T v_c $$\n",
    "\n",
    "Now:\n",
    "\n",
    "$$\\frac{\\partial J_{neg-sample}(v_c,o,U)}{\\partial v_c} = \\frac{\\partial}{\\partial s_1}\\left(−log(\\sigma(s_1))\\right)\\frac{\\partial s_1}{\\partial v_c}−\\frac{\\partial}{\\partial v_c}\\left(\\sum \\limits^{K}\\limits_{k=1}log(\\sigma(-s_2))\\right)\\frac{\\partial s_2}{\\partial v_c}$$\n",
    "\n",
    "Looking at each part separately:\n",
    "\n",
    "$$\\frac{\\partial s_1}{\\partial v_c} = u_o$$\n",
    "$$\\frac{\\partial s_2}{\\partial v_c} = u_k$$\n",
    "\n",
    "Invoke the chain rule again.\n",
    "\n",
    "$$ \\frac{\\partial}{\\partial s_1}\\left(−log(\\sigma(s_1))\\right) = - \\frac{\\partial}{\\partial y}\\left(log(y)\\right) \\frac{\\partial y}{\\partial s_1} \\text{  where  } y=\\sigma(s_1) $$\n",
    "$$ = - \\frac{1}{\\sigma(s_1)} \\sigma(s_1)(1-\\sigma(s_1)) = \\sigma(s_1)-1 $$\n",
    "\n",
    "Similarly \n",
    "\n",
    "$$−\\frac{\\partial}{\\partial v_c}\\left(\\sum \\limits^{K}\\limits_{k=1}log(\\sigma(-s_2))\\right) =\\sum \\limits^{K}\\limits_{k=1} 1-\\sigma(s_2)$$\n",
    "\n",
    "So:\n",
    "\n",
    "\n",
    "\n",
    "$$ \\frac{\\partial J_{neg-sample}(v_c,o,U)}{\\partial v_c} = (\\sigma(u_o^T v_c)-1))u_o + \\sum_k (1-\\sigma(-u_k^T v_c)) u_k $$\n",
    "\n",
    "$$ \\frac{\\partial J_{neg-sample}(v_c,o,U)}{\\partial u_o} = (\\sigma(u_o^T v_c)-1))v_c $$\n",
    "\n",
    "$$ \\frac{\\partial J_{neg-sample}(v_c,o,U)}{\\partial u_k} = (1-\\sigma(-u_k^T v_c)) v_c $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose the center word is $c=w_t$ and the context window is $[w_{t−m},...,w_{t−1},w_t,w_{t+1},...,w_{t+m}]$, where $m$ is the context window size.  Recall that for the skip-gram version of word2vec, the total loss for the context window is: \n",
    "\n",
    "$$J_{skip-gram}(v_c,w_{t−m},...w_{t+m},U) = \\sum \\limits_{\\substack{−m \\leq j \\leq m \\\\ j \\neq 0}} J(v_c,w_{t+j},U)  \\tag{8}$$ \n",
    "\n",
    "Here, $J(v_c,w_{t+j},U)$  represents  an  arbitrary  loss  term  for  the  center  word $c=w_t$ and  outside  word $w_{t+j}$. $J(v_c,w_{t+j},U)$ could be $J_{naive-softmax}(v_c,w_{t+j},U)$ or $J_{neg-sample}(v_c,w_{t+j},U)$, depending on your implementation. Write down three partial derivatives:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### i)  $\\partial J_{skip-gram}(v_c,w_{t−m},...w_{t+m},U)/\\partial U$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$ \\frac{\\partial J}{\\partial U} = \\sum_j\\frac{\\partial J(v_c,w_{t+j},U)}{\\partial U} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ii) $\\partial J_{skip-gram}(v_c,w_{t−m},...w_{t+m},U)/\\partial v_c$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial J}{\\partial v_c} = \\sum_j\\frac{\\partial J(v_c,w_{t+j},U)}{\\partial v_c} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### iii) $\\partial J_{skip-gram}(v_c,w_{t−m},...w_{t+m},U)/\\partial v_w \\text {   when   } w\\neq c$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a vector is not the centre word it has no impact on the loss. Thus the derivative is zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
